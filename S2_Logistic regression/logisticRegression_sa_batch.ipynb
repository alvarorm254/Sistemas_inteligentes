{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Functions for pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Tokenizing function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first important step is convert the comments into a list of keywords that we can analize later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') # Common words\n",
    "porter = PorterStemmer() # Getting root of words\n",
    "char3=stop[:17] # Getting 1st and 2nd person pronouns\n",
    "stop=stop[17:116]+stop[118:] # Deliting 'no' and 'not' from stop list\n",
    "# Te tokenizer function is used to get transform a comment into a list in order to be processed later.\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no', 'test', ':)', ':)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('This :) is no a <a> test! :-)</br>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 CSV into yelds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_stream=stream_docs(path='shuffled_movie_data.csv')\n",
    "docs, y = [], []\n",
    "for _ in range(50000):\n",
    "    text_aux, label =next(doc_stream)\n",
    "    text=tokenizer(text_aux)\n",
    "    docs.append(text)\n",
    "    y.append(label)\n",
    "    #print('\\n',tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Processing first positive-negative words dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply porter.stem we have repeated words in both lists, so we need deleting thouse repeated elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDuplicates(listofElements):\n",
    "    # Create an empty list to store unique elements\n",
    "    uniqueList = []\n",
    "    \n",
    "    # Iterate over the original list and for each element\n",
    "    # add it to uniqueList, if its not already there.\n",
    "    for elem in listofElements:\n",
    "        if elem not in uniqueList:\n",
    "            uniqueList.append(elem)\n",
    "    \n",
    "    # Return the list of unique elements        \n",
    "    return uniqueList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we process the positive-negative words dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007\n",
      "1391\n",
      "4783\n",
      "3160\n"
     ]
    }
   ],
   "source": [
    "positives=[line.strip() for line in open('opinion-lexicon-English/positive-words.txt')]\n",
    "positives = [porter.stem(w) for w in positives]\n",
    "print(len(positives))\n",
    "positives=removeDuplicates(positives)\n",
    "print(len(positives))\n",
    "negatives=[line.strip() for line in open('opinion-lexicon-English/negative-words.txt')]\n",
    "negatives = [porter.stem(w) for w in negatives]\n",
    "print(len(negatives))\n",
    "negatives=removeDuplicates(negatives)\n",
    "print(len(negatives))\n",
    "pron12=stop[:17]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Getting parameters referenced in links above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "(50000, 6)\n"
     ]
    }
   ],
   "source": [
    "N=50000\n",
    "X=np.zeros((N,6))\n",
    "idx=0\n",
    "for com in docs:\n",
    "    if idx%1000==0:\n",
    "        print(idx)\n",
    "    X[idx,5]+=np.log(len(com)) #len: char 6\n",
    "    for word in com:\n",
    "        X[idx,3]+=char3.count(word) #pronoun: char 4\n",
    "        if word=='!':\n",
    "            X[idx,4]=1 #! simbol: char 5\n",
    "        if (word=='no' or word=='not'):\n",
    "            X[idx,2]=1 #! simbol: char 5\n",
    "        X[idx,0]+=positives.count(word) #positive words : char 1\n",
    "        X[idx,1]+=negatives.count(word) #negative words : char 2\n",
    "    idx+=1\n",
    "\n",
    "#print(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save data data in order to avoid repeat the processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.DataFrame(X)\n",
    "df.to_csv(\"X.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Using positive-negative list base in histogram of the current dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a new list of words based in the current comments and create an input paramenter in order to the number its presence in each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['older', 'terrif', 'beauti', 'follow', 'set', 'move', 'begin', 'disney', 'thank', 'centuri', 'imag', 'season', 'solid', 'match', 'deep', 'move', 'person', 'agre', 'touch', 'york', 'awesom', 'fantasi', 'truth', 'wonder', 'portray', 'present', 'today', 'sweet', 'enjoy', 'situat', 'brother', 'lead', 'creat', 'keep', 'western', 'emot', 'beauti', 'earlier', 'creepi', 'featur', 'rare', 'offic', 'scott', 'clever', 'uniqu', 'realist', 'portray', 'fill', 'return', 'adventur', 'marri', 'masterpiec', 'favorit', 'secret', 'memor', 'danc', 'plenti', 'natur', 'romanc', 'popular', '9', 'bring', 'social', 'stun', 'japanes', 'oscar', 'anim', 'see', 'geniu', 'manag', 'edg', 'natur', 'outstand', 'perfectli', 'anim', 'outsid', '70', 'touch', 'tale', 'insid', 'mark', 'spirit', 'cold', 'polit', 'subtl', 'support', 'die', 'pace', 'larg', 'dream', 'incred', 'creat', 'appreci', 'emot', 'recent', 'busi', 'paul', 'intellig', 'variou', 'surprisingli']\n",
      "['maker', 'hardli', 'monster', 'fail', 'shoot', 'direct', 'laugh', 'grade', 'bottom', 'premis', 'lee', 'sit', 'save', 'sound', 'stupid', 'concept', 'villain', 'plain', 'weird', 'garbag', 'fairli', 'island', 'wast', 'bother', 'scienc', 'sadli', 'valu', 'posit', 'end', 'cheap', 'van', 'whatev', 'unbeliev', 'worst', 'dumb', 'pointless', 'zombi', 'excus', 'result', 'suddenli', 'sick', 'develop', 'cast', 'total', 'avoid', 'footag', 'fake', 'alien', 'attempt', 'ok', 'spend', 'poorli', 'trash', 'yeah', 'expect', 'weak', 'effort', 'wrote', '20', 'produc', 'mess', 'girlfriend', 'ridicul', 'cop', 'wors', 'consid', 'horribl', 'decid', 'terribl', 'remak', 'explain', 'bare', 'fire', 'edit', 'kill', 'neither', 'spent', 'wast', 'hair', 'badli', 'cheesi', 'okay', 'nowher', 'accent', 'fail', 'disappoint', 'produc', 'meant', 'credit', 'laugh', 'pain', 'imdb', 'filmmak', 'possibl', 'bunch', 'writer', 'wait', 'pay', 'sit', 'cover']\n"
     ]
    }
   ],
   "source": [
    "P2=100 # Number of features\n",
    "\n",
    "#We separate common words\n",
    "positive2 = pd.read_csv('data/positive.csv', index_col=0)\n",
    "positive2_i = positive2.index.values[:1000]\n",
    "stop2 = stopwords.words('english')\n",
    "positive2_i=set(positive2_i).difference(stop2)\n",
    "\n",
    "\n",
    "negative2 = pd.read_csv('data/negative.csv', index_col=0)\n",
    "negative2_i = negative2.index.values[:1000]\n",
    "negative2_i=set(negative2_i).difference(stop2)\n",
    "\n",
    "# We proced to delete repeated words.\n",
    "positive2=set(positive2_i).difference(negative2_i)\n",
    "negative2=set(negative2_i).difference(positive2_i)\n",
    "positive2=list(positive2)[:P2]\n",
    "negative2=list(negative2)[:P2]\n",
    "\n",
    "positive2 = [porter.stem(w) for w in positive2]\n",
    "negative2 = [porter.stem(w) for w in negative2]\n",
    "\n",
    "print(positive2)\n",
    "print(negative2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "(50000, 6)\n"
     ]
    }
   ],
   "source": [
    "X2=np.zeros((N,2*P2))\n",
    "idx=0\n",
    "for com in docs:\n",
    "    #if com.count('terrific'):\n",
    "        #print('foundddddd')\n",
    "    if idx%1000==0:\n",
    "        print(idx)\n",
    "    idx2=0\n",
    "    for word in positive2:\n",
    "        X2[idx,idx2]=com.count(word)\n",
    "        idx2+=1\n",
    "    #print(idx2)\n",
    "    for word in negative2:\n",
    "        X2[idx,idx2]=com.count(word)\n",
    "        idx2+=1\n",
    "    idx+=1\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Merging all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data=np.concatenate((X, X2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.asarray(y)\n",
    "y=y.reshape(y.shape[0],1)\n",
    "x_train=X_data[:40000]\n",
    "x_valid=X_data[40000:45000]\n",
    "x_test=X_data[45000:50000]\n",
    "y_train=y[:40000]\n",
    "y_valid=y[40000:45000]\n",
    "y_test=y[45000:50000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoca:  0\n",
      "--------------\n",
      "Precision:  [49.1]\n",
      "Epoca:  1000\n",
      "--------------\n",
      "Precision:  [72.94]\n",
      "Epoca:  2000\n",
      "--------------\n",
      "Precision:  [73.62]\n",
      "Epoca:  3000\n",
      "--------------\n",
      "Precision:  [74.1]\n",
      "Epoca:  4000\n",
      "--------------\n",
      "Precision:  [74.48]\n",
      "Epoca:  5000\n",
      "--------------\n",
      "Precision:  [75.02]\n",
      "Epoca:  6000\n",
      "--------------\n",
      "Precision:  [75.48]\n",
      "Epoca:  7000\n",
      "--------------\n",
      "Precision:  [75.82]\n",
      "Epoca:  8000\n",
      "--------------\n",
      "Precision:  [76.02]\n",
      "Epoca:  9000\n",
      "--------------\n",
      "Precision:  [76.06]\n",
      "Epoca:  10000\n",
      "--------------\n",
      "Precision:  [76.2]\n",
      "Epoca:  11000\n",
      "--------------\n",
      "Precision:  [76.54]\n",
      "Epoca:  12000\n",
      "--------------\n",
      "Precision:  [76.86]\n",
      "Epoca:  13000\n",
      "--------------\n",
      "Precision:  [77.08]\n",
      "Epoca:  14000\n",
      "--------------\n",
      "Precision:  [77.3]\n",
      "Epoca:  15000\n",
      "--------------\n",
      "Precision:  [77.38]\n",
      "Epoca:  16000\n",
      "--------------\n",
      "Precision:  [77.38]\n",
      "Epoca:  17000\n",
      "--------------\n",
      "Precision:  [77.52]\n",
      "Epoca:  18000\n",
      "--------------\n",
      "Precision:  [77.68]\n",
      "Epoca:  19000\n",
      "--------------\n",
      "Precision:  [77.8]\n",
      "Epoca:  20000\n",
      "--------------\n",
      "Precision:  [77.92]\n",
      "Epoca:  21000\n",
      "--------------\n",
      "Precision:  [78.06]\n",
      "Epoca:  22000\n",
      "--------------\n",
      "Precision:  [78.2]\n",
      "Epoca:  23000\n",
      "--------------\n",
      "Precision:  [78.3]\n",
      "Epoca:  24000\n",
      "--------------\n",
      "Precision:  [78.44]\n",
      "Epoca:  25000\n",
      "--------------\n",
      "Precision:  [78.58]\n",
      "Epoca:  26000\n",
      "--------------\n",
      "Precision:  [78.62]\n",
      "Epoca:  27000\n",
      "--------------\n",
      "Precision:  [78.6]\n",
      "Epoca:  28000\n",
      "--------------\n",
      "Precision:  [78.68]\n",
      "Epoca:  29000\n",
      "--------------\n",
      "Precision:  [78.76]\n",
      "Epoca:  30000\n",
      "--------------\n",
      "Precision:  [78.84]\n",
      "Epoca:  31000\n",
      "--------------\n",
      "Precision:  [78.84]\n",
      "Epoca:  32000\n",
      "--------------\n",
      "Precision:  [78.9]\n",
      "Epoca:  33000\n",
      "--------------\n",
      "Precision:  [79.]\n",
      "Epoca:  34000\n",
      "--------------\n",
      "Precision:  [79.04]\n",
      "Epoca:  35000\n",
      "--------------\n",
      "Precision:  [79.06]\n",
      "Epoca:  36000\n",
      "--------------\n",
      "Precision:  [79.12]\n",
      "Epoca:  37000\n",
      "--------------\n",
      "Precision:  [79.26]\n",
      "Epoca:  38000\n",
      "--------------\n",
      "Precision:  [79.38]\n",
      "Epoca:  39000\n",
      "--------------\n",
      "Precision:  [79.44]\n",
      "Epoca:  40000\n",
      "--------------\n",
      "Precision:  [79.4]\n",
      "Epoca:  41000\n",
      "--------------\n",
      "Precision:  [79.5]\n",
      "Epoca:  42000\n",
      "--------------\n",
      "Precision:  [79.52]\n",
      "Epoca:  43000\n",
      "--------------\n",
      "Precision:  [79.46]\n",
      "Epoca:  44000\n",
      "--------------\n",
      "Precision:  [79.54]\n",
      "Epoca:  45000\n",
      "--------------\n",
      "Precision:  [79.56]\n",
      "Epoca:  46000\n",
      "--------------\n",
      "Precision:  [79.54]\n",
      "Epoca:  47000\n",
      "--------------\n",
      "Precision:  [79.54]\n",
      "Epoca:  48000\n",
      "--------------\n",
      "Precision:  [79.56]\n",
      "Epoca:  49000\n",
      "--------------\n",
      "Precision:  [79.52]\n",
      "Epoca:  50000\n",
      "--------------\n",
      "Precision:  [79.44]\n",
      "Epoca:  51000\n",
      "--------------\n",
      "Precision:  [79.44]\n",
      "Epoca:  52000\n",
      "--------------\n",
      "Precision:  [79.44]\n",
      "Epoca:  53000\n",
      "--------------\n",
      "Precision:  [79.42]\n",
      "Epoca:  54000\n",
      "--------------\n",
      "Precision:  [79.46]\n",
      "Epoca:  55000\n",
      "--------------\n",
      "Precision:  [79.46]\n",
      "Epoca:  56000\n",
      "--------------\n",
      "Precision:  [79.48]\n",
      "Epoca:  57000\n",
      "--------------\n",
      "Precision:  [79.5]\n",
      "Epoca:  58000\n",
      "--------------\n",
      "Precision:  [79.56]\n",
      "Epoca:  59000\n",
      "--------------\n",
      "Precision:  [79.54]\n",
      "Epoca:  60000\n",
      "--------------\n",
      "Precision:  [79.52]\n",
      "Epoca:  61000\n",
      "--------------\n",
      "Precision:  [79.56]\n",
      "Epoca:  62000\n",
      "--------------\n",
      "Precision:  [79.62]\n",
      "Epoca:  63000\n",
      "--------------\n",
      "Precision:  [79.66]\n",
      "Epoca:  64000\n",
      "--------------\n",
      "Precision:  [79.72]\n",
      "Epoca:  65000\n",
      "--------------\n",
      "Precision:  [79.74]\n",
      "Epoca:  66000\n",
      "--------------\n",
      "Precision:  [79.72]\n",
      "Epoca:  67000\n",
      "--------------\n",
      "Precision:  [79.72]\n",
      "Epoca:  68000\n",
      "--------------\n",
      "Precision:  [79.72]\n",
      "Epoca:  69000\n",
      "--------------\n",
      "Precision:  [79.72]\n",
      "Epoca:  70000\n",
      "--------------\n",
      "Precision:  [79.64]\n",
      "Epoca:  71000\n",
      "--------------\n",
      "Precision:  [79.64]\n",
      "Epoca:  72000\n",
      "--------------\n",
      "Precision:  [79.68]\n",
      "Epoca:  73000\n",
      "--------------\n",
      "Precision:  [79.72]\n",
      "Epoca:  74000\n",
      "--------------\n",
      "Precision:  [79.72]\n",
      "Epoca:  75000\n",
      "--------------\n",
      "Precision:  [79.78]\n",
      "Epoca:  76000\n",
      "--------------\n",
      "Precision:  [79.74]\n",
      "Epoca:  77000\n",
      "--------------\n",
      "Precision:  [79.78]\n",
      "Epoca:  78000\n",
      "--------------\n",
      "Precision:  [79.76]\n",
      "Epoca:  79000\n",
      "--------------\n",
      "Precision:  [79.72]\n",
      "Epoca:  80000\n",
      "--------------\n",
      "Precision:  [79.7]\n",
      "Epoca:  81000\n",
      "--------------\n",
      "Precision:  [79.72]\n",
      "Epoca:  82000\n",
      "--------------\n",
      "Precision:  [79.7]\n",
      "Epoca:  83000\n",
      "--------------\n",
      "Precision:  [79.74]\n",
      "Epoca:  84000\n",
      "--------------\n",
      "Precision:  [79.78]\n",
      "Epoca:  85000\n",
      "--------------\n",
      "Precision:  [79.78]\n",
      "Epoca:  86000\n",
      "--------------\n",
      "Precision:  [79.76]\n",
      "Epoca:  87000\n",
      "--------------\n",
      "Precision:  [79.76]\n",
      "Epoca:  88000\n",
      "--------------\n",
      "Precision:  [79.82]\n",
      "Epoca:  89000\n",
      "--------------\n",
      "Precision:  [79.82]\n",
      "Epoca:  90000\n",
      "--------------\n",
      "Precision:  [79.84]\n",
      "Epoca:  91000\n",
      "--------------\n",
      "Precision:  [79.84]\n",
      "Epoca:  92000\n",
      "--------------\n",
      "Precision:  [79.88]\n",
      "Epoca:  93000\n",
      "--------------\n",
      "Precision:  [79.88]\n",
      "Epoca:  94000\n",
      "--------------\n",
      "Precision:  [79.88]\n",
      "Epoca:  95000\n",
      "--------------\n",
      "Precision:  [79.84]\n",
      "Epoca:  96000\n",
      "--------------\n",
      "Precision:  [79.84]\n",
      "Epoca:  97000\n",
      "--------------\n",
      "Precision:  [79.82]\n",
      "Epoca:  98000\n",
      "--------------\n",
      "Precision:  [79.84]\n",
      "Epoca:  99000\n",
      "--------------\n",
      "Precision:  [79.86]\n",
      "[[ 0.11367074 -0.10475283 -0.5536972  -0.02883532  0.00335579  0.01737839\n",
      "   0.07223783  0.21165317  0.17050804  0.17780842  0.04550327  0.10135656\n",
      "   0.13227717 -0.03212169  0.11814279  0.06482978  0.12789554  0.11246058\n",
      "   0.17733838  0.02537257  0.10656484  0.10076315  0.00318281 -0.03394986\n",
      "   0.25658691  0.09094111  0.1886095  -0.0305796   0.11502503  0.14168433\n",
      "   0.10154864  0.13434901  0.42200954  0.14043423  0.31963719  0.16014148\n",
      "   0.14555987 -0.15177911  0.03969021  0.14630448  0.06186111  0.15449598\n",
      "   0.1693238   0.01091101  0.19167082  0.0183376   0.21445897  0.1335415\n",
      "  -0.01734755  0.02480915  0.24964053  0.23507948  0.10170652 -0.02269214\n",
      "   0.17741335  0.10065491  0.08703663  0.23471264  0.4747966   0.12705899\n",
      "   0.13199043 -0.01756904  0.09181347  0.11145554  0.02951488 -0.0027617\n",
      "   0.27623362  0.09356694  0.12955163  0.1626107   0.05122227  0.15715681\n",
      "   0.03038667  0.12591518  0.14012663 -0.0181851   0.21873454  0.11211794\n",
      "   0.20246434  0.22439387  0.02733252  0.05908416  0.02647408  0.25683594\n",
      "   0.23265705  0.12876549  0.040883    0.00786039  0.13896464  0.02536302\n",
      "   0.2286766   0.06732021  0.08123751  0.09457116  0.03219735  0.17036025\n",
      "   0.10810704  0.03841656  0.14673049  0.15481065  0.17197918  0.02229288\n",
      "   0.08207735  0.01382526  0.00237974  0.12486967 -0.12942762 -0.13736324\n",
      "  -0.05730467 -0.33446111 -0.08631859 -0.11305379  0.01480392 -0.16314758\n",
      "  -0.11365602 -0.24933528 -0.11528564 -0.1356861  -0.54045638 -0.2829072\n",
      "  -0.47829516 -0.0793423   0.03085364 -0.12339461  0.00242957 -0.21509918\n",
      "  -0.11453223 -0.03686631 -0.74256172 -0.2903873  -0.08720065 -0.11601132\n",
      "  -0.14246388 -0.16942486  0.01900799 -0.29506847 -0.1499442  -0.11778079\n",
      "  -0.1910514  -1.0447218  -0.20241114 -0.27593556  0.02083791 -0.19911148\n",
      "  -0.06340671 -0.04561261  0.04226662 -0.16490986 -0.0716095  -0.20549729\n",
      "  -0.35737628 -0.15518375 -0.08274237 -0.00245781 -0.30262661 -0.36452883\n",
      "  -0.20708536 -0.41418359 -0.15234896 -0.15806017 -0.03427201 -0.19595228\n",
      "  -0.25682113 -0.08115951 -0.1115772  -0.11662235 -0.3314854   0.02285579\n",
      "  -0.39655195  0.05384945 -0.5682006  -0.05002971 -0.51104085 -0.09122936\n",
      "  -0.61650383 -0.18538655 -0.1764208  -0.22162449 -0.02919138 -0.17086887\n",
      "   0.1333115  -0.2125231  -0.16599969 -0.74165089 -0.12467783 -0.28979903\n",
      "  -0.1096714  -0.28121695 -0.171748   -0.21799988 -0.33626016 -0.46172317\n",
      "  -0.11359265 -0.08121328 -0.10904446  0.01532338 -0.12375969 -0.0690363\n",
      "  -0.12555557 -0.09748544 -0.22456478 -0.28016644 -0.07003636 -0.10278821\n",
      "  -0.13602635 -0.0625331 ]]\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# BATCH\n",
    "############################################\n",
    "alfa=0.001\n",
    "reg=0.002\n",
    "epochs=100000\n",
    "W=np.random.rand(1,x_train.shape[1])/x_train.shape[1]\n",
    "bias=np.random.rand(1,1)/x_train.shape[1]\n",
    "W2=W*1\n",
    "prec=0\n",
    "ep=0\n",
    "for epoch in range(epochs):\n",
    "    err=np.transpose(y_train)-sigmoid(bias+np.matmul(W2,np.transpose(x_train)))\n",
    "    dw=alfa*np.matmul(err,x_train)/x_train.shape[0]\n",
    "    W2+=dw\n",
    "    if epoch%1000==0:\n",
    "        y_pred=np.round(sigmoid(np.matmul(x_valid,np.transpose(W2))))\n",
    "        precision=100*(1-sum(abs(y_pred-y_valid))/y_pred.shape[0])\n",
    "        print('Epoca: ',epoch)\n",
    "        print('--------------')\n",
    "        print('Precision: ',precision)\n",
    "        if prec<precision:\n",
    "            prec=precision\n",
    "            ep=epoch\n",
    "            W=W2\n",
    "\n",
    "print(W2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Getting results in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 40000)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err=np.transpose(y_train)-sigmoid(bias+np.matmul(W2,np.transpose(x_train)))\n",
    "err.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1896,  572],\n",
       "       [ 461, 2071]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred2=np.round(sigmoid(np.matmul(x_test,np.transpose(W))))\n",
    "confusion_matrix(y_test, y_pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([79.86])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
